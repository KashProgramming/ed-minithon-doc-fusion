{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, LSTM, Dropout, Embedding, Activation\n",
    "from tensorflow.keras.layers import add, concatenate, BatchNormalization, Input\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .', 'A little girl climbing the stairs to her playhouse .', 'A little girl in a pink dress going into a wooden cabin .']\n"
     ]
    }
   ],
   "source": [
    "# using a kaggle dataset to train the model to generate image captions\n",
    "def load_description(text): \n",
    "    mapping = dict() \n",
    "    for line in text.split(\"\\n\"): \n",
    "        token = line.split(\"\\t\") \n",
    "        if len(line)<2:   # remove short descriptions \n",
    "            continue\n",
    "        img_id = token[0].split(\".\")[0] # name of the image\n",
    "        img_des = token[1]              # description of the image\n",
    "        if img_id not in mapping:\n",
    "            mapping[img_id] = list()\n",
    "        mapping[img_id].append(img_des)\n",
    "    return mapping\n",
    "token_path = \"/Users/kashishmandhane/Documents/Kashish Data/LAPTOP STUFF/DJ Sanghvi College/Extra-curriculars/Hackathons/Ed-tech/Flickr8K/Flickr8k_text/Flickr8k.token.txt\"\n",
    "text = open(token_path, \"r\", encoding = \"utf-8\").read()\n",
    "descriptions = load_description(text)\n",
    "print(descriptions[\"1000268201_693b08cb0e\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.text_clean import clean_text\n",
    "# for key, des_list in descriptions.items():\n",
    "#     clean_text(des_list)\n",
    "# # Now, the dictionary values (lists) will be cleaned\n",
    "# print(descriptions[\"1000268201_693b08cb0e\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_vocab(desc): \n",
    "\twords = set() \n",
    "\tfor key in desc.keys(): \n",
    "\t\tfor line in desc[key]: \n",
    "\t\t\twords.update(line.split()) \n",
    "\treturn words \n",
    "vocab = to_vocab(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = \"/Users/kashishmandhane/Documents/Kashish Data/LAPTOP STUFF/DJ Sanghvi College/Extra-curriculars/Hackathons/Ed-tech/Flickr8K/Flicker8k_Images\"\n",
    "img = glob.glob(images + \"*.jpg\") # list of image names in folder\n",
    "\n",
    "train_path = \"/Users/kashishmandhane/Documents/Kashish Data/LAPTOP STUFF/DJ Sanghvi College/Extra-curriculars/Hackathons/Ed-tech/Flickr8K/Flickr8k_text/Flickr_8k.trainImages.txt\"\n",
    "train_images = open(train_path, \"r\", encoding = \"utf-8\").read().split(\"\\n\")\n",
    "train_img = [] # list of all images in training set\n",
    "for im in img:\n",
    "\tif(im[len(images):] in train_images):\n",
    "\t\ttrain_img.append(im)\n",
    "# load descriptions of training set in a dictionary. Name of the image will act as key \n",
    "def load_clean_descriptions(des, dataset):\n",
    "\tdataset_des = dict()\n",
    "\tfor key, des_list in des.items():\n",
    "\t\tif key+\".jpg\" in dataset:\n",
    "\t\t\tif key not in dataset_des:\n",
    "\t\t\t\tdataset_des[key] = list()\n",
    "\t\t\tfor line in des_list:\n",
    "\t\t\t\tdesc = \"startseq \" + line + \" endseq\"\n",
    "\t\t\t\tdataset_des[key].append(desc)\n",
    "\treturn dataset_des\n",
    "train_descriptions = load_clean_descriptions(descriptions, train_images) \n",
    "print(train_descriptions['1000268201_693b08cb0e'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(img_path):\n",
    "    # inception v3 excepts img in 299 * 299 * 3\n",
    "    img = load_img(img_path, target_size = (299, 299))\n",
    "    x = img_to_array(img)\n",
    "    # Add one more dimension\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "def encode(image):\n",
    "    image = preprocess_img(image)\n",
    "    vec = model.predict(image)\n",
    "    vec = np.reshape(vec, (vec.shape[1]))\n",
    "    return vec\n",
    "  \n",
    "base_model = InceptionV3(weights = \"imagenet\") \n",
    "model = Model(base_model.input, base_model.layers[-2].output) \n",
    "# run the encode function on all train images and store the feature vectors in a list \n",
    "encoding_train = {} \n",
    "for img in train_img:\n",
    "    encoding_train[img[len(images):]] = encode(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all training captions \n",
    "all_train_captions = [] \n",
    "for key, val in train_descriptions.items(): \n",
    "\tfor caption in val:\n",
    "\t\tall_train_captions.append(caption)\n",
    "# consider only words which occur atleast 10 times \n",
    "vocabulary = vocab \n",
    "threshold = 10 # you can change this value according to your need \n",
    "word_counts = {} \n",
    "for cap in all_train_captions: \n",
    "\tfor word in cap.split(' '): \n",
    "\t\tword_counts[word] = word_counts.get(word, 0) + 1\n",
    "vocab = [word for word in word_counts if word_counts[word] >= threshold]\n",
    "# word mapping to integers\n",
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "ix = 1\n",
    "for word in vocab:\n",
    "\twordtoix[word] = ix\n",
    "\tixtoword[ix] = word\n",
    "\tix += 1\t\n",
    "# find the maximum length of a description in a dataset \n",
    "max_length = max(len(des.split()) for des in all_train_captions) \n",
    "max_length"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
